{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7884e93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder, util\n",
    "import torch\n",
    "import hnswlib\n",
    "from transformers import AutoModel\n",
    "\n",
    "if os.path.isfile(\"rag_instruct.json\"): \n",
    "    df = pd.read_json(\"rag_instruct.json\")\n",
    "else:\n",
    "    df = pd.read_json(\"hf://datasets/FreedomIntelligence/RAG-Instruct/rag_instruct.json\")\n",
    "\n",
    "documents = df['documents']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7906762",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 512, 'do_lower_case': False}) with Transformer model: BertModel \n",
       "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
       "  (2): Normalize()\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "semb_model = SentenceTransformer('multi-qa-MiniLM-L6-cos-v1')\n",
    "xenc_model = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
    "semb_model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23331b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#corpus_embeddings = semb_model.encode(documents, convert_to_tensor=True, show_progress_bar=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed2cb2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "#size_corpus = len(corpus_embeddings)\n",
    "#print(\"Size of corpus: \", size_corpus)\n",
    "index = hnswlib.Index(space='cosine', dim=40541)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9a74a1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading index...\n"
     ]
    }
   ],
   "source": [
    "# Define hnswlib index path\n",
    "index_path = \"./hnswlib.index\"\n",
    "\n",
    "# Load index if available\n",
    "if os.path.exists(index_path):\n",
    "    print(\"Loading index...\")\n",
    "    index.load_index(index_path)\n",
    "# Else index data collection\n",
    "else:\n",
    "    # Initialise the index\n",
    "    print(\"Start creating HNSWLIB index\")\n",
    "    index.init_index(max_elements=corpus_embeddings.size(0), ef_construction=400, M=64)\n",
    "    #  Compute the HNSWLIB index (it may take a while)\n",
    "    index.add_items(corpus_embeddings.cpu(), list(range(len(corpus_embeddings))))\n",
    "    # Save the index to a file for future loading\n",
    "    print(\"Saving index to:\", index_path)\n",
    "    index.save_index(index_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3ce4461",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get the related docs\n",
    "def get_related_docs(query, k=3):\n",
    "    query_embedding = semb_model.encode(query, convert_to_tensor=True)\n",
    "    corpus_ids, _ = index.knn_query(query_embedding.cpu(), k=k)\n",
    "\n",
    "    model_inputs = [(query, str(documents[idx])) for idx in corpus_ids[0]]\n",
    "    cross_scores = xenc_model.predict(model_inputs)\n",
    "    send_to_LLM = \"\"\n",
    "    positive_docs = [documents[corpus_ids[0][idx]] for idx in np.argsort(-cross_scores) if cross_scores[idx] > 0]\n",
    "\n",
    "    if len(positive_docs) > 1:\n",
    "        for i, doc in enumerate(positive_docs):\n",
    "            send_to_LLM += f\"Document {i+1}:\\n\\n\"\n",
    "            # Convert the list 'doc' to a string before concatenating\n",
    "            send_to_LLM += str(doc) + \"\\n\"\n",
    "    elif len(positive_docs) == 1:\n",
    "        # Convert the list to a string if there's only one document\n",
    "        send_to_LLM = str(positive_docs[0])\n",
    "\n",
    "    else:\n",
    "        # If no positive scores, take the top 2 negative scores\n",
    "        negative_docs = []\n",
    "        for idx in np.argsort(-cross_scores)[:2]: # Take the top 2 indices based on sorted scores\n",
    "            negative_docs.append(documents[corpus_ids[0][idx]])\n",
    "\n",
    "        if len(negative_docs) > 1:\n",
    "            for i, doc in enumerate(negative_docs):\n",
    "                send_to_LLM += f\"Document {i+1}:\\n\"\n",
    "                send_to_LLM += str(doc) + \"\\n\\n\"\n",
    "        elif len(negative_docs) == 1:\n",
    "            send_to_LLM = str(negative_docs[0])\n",
    "\n",
    "    return send_to_LLM\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c43da1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6745b204e428438c81fd87601f4f4272",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,  # or bfloat16 if supported\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n",
    "\n",
    "# Load model with quantization\n",
    "llm_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"AITeamVN/Vi-Qwen2-3B-RAG\",\n",
    "    quantization_config=quant_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "llm_tokenizer = AutoTokenizer.from_pretrained(\"AITeamVN/Vi-Qwen2-3B-RAG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abef699",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"Do all plants do photosynthesis?\"\n",
    "\n",
    "context_docs = get_related_docs(query)\n",
    "\n",
    "prompt = f\"Given this context: \\n{context_docs} \\n\\nPlease answer the question: {query}.\\n\\nAnswer:\\n\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        input_ids=inputs[\"input_ids\"].to(model.device),\n",
    "        attention_mask=inputs[\"attention_mask\"].to(model.device),\n",
    "        max_new_tokens=200,\n",
    "        temperature=0.7,\n",
    "        top_p=0.9,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "# Decode and print result\n",
    "answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(\"\\n=== Generated Answer ===\\n\")\n",
    "print(answer.split(\"Answer:\")[-1].strip())  # Optional: strip prompt parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0cf7a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(query):\n",
    "    context_docs = get_related_docs(query)\n",
    "\n",
    "    prompt = f\"Given this context: \\n{context_docs} \\n\\nPlease answer the question: {query}.\\n\\nAnswer:\\n\"\n",
    "\n",
    "    inputs = llm_tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = llm_model.generate(\n",
    "            input_ids=inputs[\"input_ids\"].to(llm_model.device),\n",
    "            attention_mask=inputs[\"attention_mask\"].to(llm_model.device),\n",
    "            max_new_tokens=200,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            pad_token_id=llm_tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    # Decode and print result\n",
    "    answer = llm_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(\"\\n=== Generated Answer ===\\n\")\n",
    "    return answer.split(\"Answer:\")[-1].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc569c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ecc365a675c34117b168b69684e02f55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,  # or bfloat16 if supported\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n",
    "\n",
    "# Load model with quantization\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"AITeamVN/Vi-Qwen2-3B-RAG\",\n",
    "    quantization_config=quant_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"AITeamVN/Vi-Qwen2-3B-RAG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c08fecc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "import webrtcvad\n",
    "import whisper\n",
    "import collections\n",
    "import struct\n",
    "import re\n",
    "from transformers import WhisperForConditionalGeneration, WhisperProcessor, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "SAMPLE_RATE = 16000\n",
    "FRAME_DURATION_MS = 30  # ms\n",
    "FRAME_SIZE = int(SAMPLE_RATE * FRAME_DURATION_MS / 1000)\n",
    "CHANNELS = 1\n",
    "VAD_AGGRESSIVENESS = 2  # 0–3: higher = more aggressive\n",
    "MAX_SILENCE_SECONDS = 1.0\n",
    "\n",
    "# === Load Whisper ===\n",
    "model_id = \"openai/whisper-base\"  # You can also try \"small\", \"medium\", etc.\n",
    "\n",
    "processor = WhisperProcessor.from_pretrained(model_id)\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=quant_config\n",
    ")\n",
    "\n",
    "# === Setup VAD ===\n",
    "vad = webrtcvad.Vad(VAD_AGGRESSIVENESS) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a46ad3",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "GOODBYE_PATTERNS = [\n",
    "    r\"\\b(bye|goodbye|see you|exit|quit|farewell)\\b\",\n",
    "    r\"talk to you later\",\n",
    "    r\"that's all\",\n",
    "    r\"that's it\"\n",
    "]\n",
    "\n",
    "def float32_to_int16(audio):\n",
    "    return (audio * 32767).astype(np.int16)\n",
    "\n",
    "def is_speech(frame_bytes):\n",
    "    return vad.is_speech(frame_bytes, SAMPLE_RATE)\n",
    "\n",
    "def is_goodbye(text):\n",
    "    for pattern in GOODBYE_PATTERNS:\n",
    "        if re.search(pattern, text, re.IGNORECASE):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def record_until_silence():\n",
    "    print(\"Listening...\")\n",
    "\n",
    "    buffer = []\n",
    "    silence_buffer = collections.deque(maxlen=int(MAX_SILENCE_SECONDS * 1000 / FRAME_DURATION_MS))\n",
    "    stream = sd.InputStream(samplerate=SAMPLE_RATE, channels=CHANNELS, dtype='float32', blocksize=FRAME_SIZE)\n",
    "\n",
    "    with stream:\n",
    "        while True:\n",
    "            audio_chunk, _ = stream.read(FRAME_SIZE)\n",
    "            audio_chunk = audio_chunk.flatten()\n",
    "            audio_int16 = float32_to_int16(audio_chunk)\n",
    "            frame_bytes = struct.pack(f\"{len(audio_int16)}h\", *audio_int16)\n",
    "\n",
    "            if is_speech(frame_bytes):\n",
    "                buffer.append(audio_chunk)\n",
    "                silence_buffer.clear()\n",
    "            else:\n",
    "                silence_buffer.append(audio_chunk)\n",
    "                if len(silence_buffer) == silence_buffer.maxlen and len(buffer) > 0:\n",
    "                    print(\"Silence detected, stopping...\")\n",
    "                    break\n",
    "\n",
    "    full_audio = np.concatenate(buffer)\n",
    "    return full_audio\n",
    "\n",
    "\n",
    "def transcribe_audio_array(audio_array: np.ndarray, sampling_rate: int):\n",
    "    if audio_array.ndim > 1:\n",
    "        audio_array = audio_array.mean(axis=1)  # convert stereo to mono\n",
    "\n",
    "    # Convert to tensor\n",
    "    waveform = torch.tensor(audio_array, dtype=torch.float32)\n",
    "\n",
    "    # Resample if needed\n",
    "    if sampling_rate != 16000:\n",
    "        import torchaudio\n",
    "        waveform = torchaudio.functional.resample(waveform, orig_freq=sampling_rate, new_freq=16000)\n",
    "\n",
    "    # Whisper expects float32 here — processor handles float32, model input will be auto-cast later\n",
    "    inputs = processor(waveform, sampling_rate=16000, return_tensors=\"pt\")\n",
    "    input_features = inputs.input_features.to(model.device, dtype=torch.float16)  # ✅ cast to float16\n",
    "\n",
    "    with torch.no_grad():\n",
    "        predicted_ids = model.generate(input_features)\n",
    "    transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]\n",
    "    return transcription\n",
    "\n",
    "\n",
    "# === Conversation Loop ===\n",
    "def start_conversation():\n",
    "    generated_answer = generate_response(\"what is photosinthesis?\")\n",
    "    print(f\"AI: {generated_answer}\")\n",
    "    return\n",
    "    print(\"Start speaking. Say 'goodbye' to end the conversation.\")\n",
    "    while True:\n",
    "        audio = record_until_silence()\n",
    "        if len(audio) == 0:\n",
    "            continue  # skip empty audio\n",
    "\n",
    "        print(\"Transcribing...\")\n",
    "        \n",
    "        # Decode\n",
    "        text = transcribe_audio_array(audio, sampling_rate=SAMPLE_RATE)\n",
    "        if is_goodbye(text):\n",
    "            print(\"Goodbye detected. Ending conversation.\")\n",
    "            break\n",
    "\n",
    "        generated_answer = generate_response(text)\n",
    "        print(f\"AI: {generated_answer}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b9a4816",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_conversation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f437bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7897c30f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7884e93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sentence_transformers import SentenceTransformer, CrossEncoder, util\n",
    "import torch\n",
    "import hnswlib\n",
    "from transformers import AutoModel\n",
    "\n",
    "if os.path.isfile(\"rag_instruct.json\"): \n",
    "    df = pd.read_json(\"rag_instruct.json\")\n",
    "else:\n",
    "    df = pd.read_json(\"hf://datasets/FreedomIntelligence/RAG-Instruct/rag_instruct.json\")\n",
    "\n",
    "documents = df['documents']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7906762",
   "metadata": {},
   "outputs": [],
   "source": [
    "semb_model = SentenceTransformer('multi-qa-MiniLM-L6-cos-v1')\n",
    "xenc_model = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
    "#semb_model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23331b4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "626a2977f9694e7885216d85d68f17ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1267 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "corpus_embeddings = semb_model.encode(documents, convert_to_tensor=True, show_progress_bar=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed2cb2df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of corpus:  40541\n"
     ]
    }
   ],
   "source": [
    "size_corpus = len(corpus_embeddings)\n",
    "print(\"Size of corpus: \", size_corpus)\n",
    "index = hnswlib.Index(space='cosine', dim=corpus_embeddings.size(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9a74a1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start creating HNSWLIB index\n",
      "Saving index to: ./hnswlib.index\n"
     ]
    }
   ],
   "source": [
    "# Define hnswlib index path\n",
    "index_path = \"./hnswlib.index\"\n",
    "\n",
    "# Load index if available\n",
    "if os.path.exists(index_path):\n",
    "    print(\"Loading index...\")\n",
    "    index.load_index(index_path)\n",
    "# Else index data collection\n",
    "else:\n",
    "    # Initialise the index\n",
    "    print(\"Start creating HNSWLIB index\")\n",
    "    index.init_index(max_elements=corpus_embeddings.size(0), ef_construction=400, M=64)\n",
    "    #  Compute the HNSWLIB index (it may take a while)\n",
    "    index.add_items(corpus_embeddings.cpu(), list(range(len(corpus_embeddings))))\n",
    "    # Save the index to a file for future loading\n",
    "    print(\"Saving index to:\", index_path)\n",
    "    index.save_index(index_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3ce4461",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get the related docs\n",
    "def get_related_docs(query, k=3):\n",
    "    query_embedding = semb_model.encode(query, convert_to_tensor=True)\n",
    "    corpus_ids, _ = index.knn_query(query_embedding.cpu(), k=k)\n",
    "\n",
    "    model_inputs = [(query, str(documents[idx])) for idx in corpus_ids[0]]\n",
    "    cross_scores = xenc_model.predict(model_inputs)\n",
    "    send_to_LLM = \"\"\n",
    "    positive_docs = [documents[corpus_ids[0][idx]] for idx in np.argsort(-cross_scores) if cross_scores[idx] > 0]\n",
    "\n",
    "    if len(positive_docs) > 1:\n",
    "        for i, doc in enumerate(positive_docs):\n",
    "            send_to_LLM += f\"Document {i+1}:\\n\\n\"\n",
    "            # Convert the list 'doc' to a string before concatenating\n",
    "            send_to_LLM += str(doc) + \"\\n\"\n",
    "    elif len(positive_docs) == 1:\n",
    "        # Convert the list to a string if there's only one document\n",
    "        send_to_LLM = str(positive_docs[0])\n",
    "\n",
    "    else:\n",
    "        # If no positive scores, take the top 2 negative scores\n",
    "        negative_docs = []\n",
    "        for idx in np.argsort(-cross_scores)[:2]: # Take the top 2 indices based on sorted scores\n",
    "            negative_docs.append(documents[corpus_ids[0][idx]])\n",
    "\n",
    "        if len(negative_docs) > 1:\n",
    "            for i, doc in enumerate(negative_docs):\n",
    "                send_to_LLM += f\"Document {i+1}:\\n\"\n",
    "                send_to_LLM += str(doc) + \"\\n\\n\"\n",
    "        elif len(negative_docs) == 1:\n",
    "            send_to_LLM = str(negative_docs[0])\n",
    "\n",
    "    return send_to_LLM\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5c43da1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65e52bcc8b5a4f269909051c274e6ef5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,  # or bfloat16 if supported\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n",
    "\n",
    "# Load model with quantization\n",
    "llm_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"AITeamVN/Vi-Qwen2-3B-RAG\",\n",
    "    quantization_config=quant_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "llm_tokenizer = AutoTokenizer.from_pretrained(\"AITeamVN/Vi-Qwen2-3B-RAG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0cf7a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(query):\n",
    "    context_docs = get_related_docs(query)\n",
    "\n",
    "    prompt = f\"Given this context: \\n{context_docs} \\n\\nPlease answer the question: {query}.\\n\\nAnswer:\\n\"\n",
    "\n",
    "    inputs = llm_tokenizer(prompt, return_tensors=\"pt\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = llm_model.generate(\n",
    "            input_ids=inputs[\"input_ids\"].to(llm_model.device),\n",
    "            attention_mask=inputs[\"attention_mask\"].to(llm_model.device),\n",
    "            max_new_tokens=200,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            pad_token_id=llm_tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    # Decode and print result\n",
    "    answer = llm_tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(\"\\n=== Generated Answer ===\\n\")\n",
    "    return answer.split(\"Answer:\")[-1].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4bc569c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d244100ceba440fb01f90d1afe455f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "quant_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,  # or bfloat16 if supported\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ")\n",
    "\n",
    "# Load model with quantization\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"AITeamVN/Vi-Qwen2-3B-RAG\",\n",
    "    quantization_config=quant_config,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"AITeamVN/Vi-Qwen2-3B-RAG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c08fecc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "import webrtcvad\n",
    "import whisper\n",
    "import collections\n",
    "import struct\n",
    "import re\n",
    "from transformers import WhisperForConditionalGeneration, WhisperProcessor, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "SAMPLE_RATE = 16000\n",
    "FRAME_DURATION_MS = 30  # ms\n",
    "FRAME_SIZE = int(SAMPLE_RATE * FRAME_DURATION_MS / 1000)\n",
    "CHANNELS = 1\n",
    "VAD_AGGRESSIVENESS = 2  # 0–3: higher = more aggressive\n",
    "MAX_SILENCE_SECONDS = 1.0\n",
    "\n",
    "# === Load Whisper ===\n",
    "model_id = \"openai/whisper-base\"  # You can also try \"small\", \"medium\", etc.\n",
    "\n",
    "processor = WhisperProcessor.from_pretrained(model_id)\n",
    "model = WhisperForConditionalGeneration.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=quant_config\n",
    ")\n",
    "\n",
    "# === Setup VAD ===\n",
    "vad = webrtcvad.Vad(VAD_AGGRESSIVENESS) \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a46ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "GOODBYE_PATTERNS = [\n",
    "    r\"\\b(bye|goodbye|see you|exit|quit|farewell)\\b\",\n",
    "    r\"talk to you later\",\n",
    "    r\"that's all\",\n",
    "    r\"that's it\"\n",
    "]\n",
    "\n",
    "def float32_to_int16(audio):\n",
    "    return (audio * 32767).astype(np.int16)\n",
    "\n",
    "def is_speech(frame_bytes):\n",
    "    return vad.is_speech(frame_bytes, SAMPLE_RATE)\n",
    "\n",
    "def is_goodbye(text):\n",
    "    for pattern in GOODBYE_PATTERNS:\n",
    "        if re.search(pattern, text, re.IGNORECASE):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "def record_until_silence():\n",
    "    print(\"Listening...\")\n",
    "\n",
    "    buffer = []\n",
    "    silence_buffer = collections.deque(maxlen=int(MAX_SILENCE_SECONDS * 1000 / FRAME_DURATION_MS))\n",
    "    stream = sd.InputStream(samplerate=SAMPLE_RATE, channels=CHANNELS, dtype='float32', blocksize=FRAME_SIZE)\n",
    "\n",
    "    with stream:\n",
    "        while True:\n",
    "            audio_chunk, _ = stream.read(FRAME_SIZE)\n",
    "            audio_chunk = audio_chunk.flatten()\n",
    "            audio_int16 = float32_to_int16(audio_chunk)\n",
    "            frame_bytes = struct.pack(f\"{len(audio_int16)}h\", *audio_int16)\n",
    "\n",
    "            if is_speech(frame_bytes):\n",
    "                buffer.append(audio_chunk)\n",
    "                silence_buffer.clear()\n",
    "            else:\n",
    "                silence_buffer.append(audio_chunk)\n",
    "                if len(silence_buffer) == silence_buffer.maxlen and len(buffer) > 0:\n",
    "                    print(\"Silence detected, stopping...\")\n",
    "                    break\n",
    "\n",
    "    full_audio = np.concatenate(buffer)\n",
    "    return full_audio\n",
    "\n",
    "\n",
    "def transcribe_audio_array(audio_array: np.ndarray, sampling_rate: int):\n",
    "    if audio_array.ndim > 1:\n",
    "        audio_array = audio_array.mean(axis=1)  # convert stereo to mono\n",
    "\n",
    "    # Convert to tensor\n",
    "    waveform = torch.tensor(audio_array, dtype=torch.float32)\n",
    "\n",
    "    # Resample if needed\n",
    "    if sampling_rate != 16000:\n",
    "        import torchaudio\n",
    "        waveform = torchaudio.functional.resample(waveform, orig_freq=sampling_rate, new_freq=16000)\n",
    "\n",
    "    # Whisper expects float32 here — processor handles float32, model input will be auto-cast later\n",
    "    inputs = processor(waveform, sampling_rate=16000, return_tensors=\"pt\")\n",
    "    input_features = inputs.input_features.to(model.device, dtype=torch.float16)  # ✅ cast to float16\n",
    "\n",
    "    with torch.no_grad():\n",
    "        predicted_ids = model.generate(input_features)\n",
    "    transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]\n",
    "    return transcription\n",
    "\n",
    "\n",
    "# === Conversation Loop ===\n",
    "def start_conversation():\n",
    "    print(\"Start speaking. Say 'goodbye' to end the conversation.\")\n",
    "    while True:\n",
    "        audio = record_until_silence()\n",
    "        if len(audio) == 0:\n",
    "            continue  # skip empty audio\n",
    "\n",
    "        print(\"Transcribing...\")\n",
    "        \n",
    "        # Decode\n",
    "        text = transcribe_audio_array(audio, sampling_rate=SAMPLE_RATE)[:50]\n",
    "\n",
    "        print(f\"You have said: {text}\")\n",
    "        if is_goodbye(text):\n",
    "            print(\"Goodbye detected. Ending conversation.\")\n",
    "            break\n",
    "\n",
    "        generated_answer = generate_response(text)\n",
    "        print(f\"AI: {generated_answer}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7b9a4816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start speaking. Say 'goodbye' to end the conversation.\n",
      "Listening...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Due to a bug fix in https://github.com/huggingface/transformers/pull/28687 transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English.This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silence detected, stopping...\n",
      "Transcribing...\n",
      "\n",
      "=== Generated Answer ===\n",
      "\n",
      "AI: Photosynthesis is a process used by plants and other organisms to convert light energy into chemical energy that can later be released to fuel the organisms' activities. This chemical energy is stored in carbohydrate molecules, such as sugars, which are synthesized from carbon dioxide and water. Oxygen is also released as a waste product in most cases.\n",
      "\n",
      "In more detail:\n",
      "- Photosynthesis occurs in the chloroplasts of plant cells.\n",
      "- Water (HO) and carbon dioxide (CO₂) are used as raw materials.\n",
      "- Energy from light is captured and converted into chemical energy, primarily stored in the form of glucose.\n",
      "- The process can be divided into two main stages: the light-dependent reactions and the light-independent reactions (Calvin cycle).\n",
      "- In the light-dependent reactions, light energy is used to split water molecules, releasing oxygen and creating ATP (adenosine triphosphate) and NADPH (nicotinamide adenine dinucleotide phosphate).\n",
      "- In the Calvin cycle, carbon dioxide is fixed into\n",
      "Listening...\n",
      "Silence detected, stopping...\n",
      "Transcribing...\n",
      "\n",
      "=== Generated Answer ===\n",
      "\n",
      "AI: Based on the context provided, a school can be defined as follows:\n",
      "\n",
      "A school is an institution providing education, usually for pupils from primary or secondary grades. Schools may be organized differently depending on the country, culture, and type of education offered. They serve as a place where students interact with teachers, participate in various subjects, and develop their intellectual, social, and emotional skills.\n",
      "\n",
      "Some key characteristics of a school include:\n",
      "\n",
      "1. Providing educational opportunities: Schools aim to equip students with knowledge and skills necessary for personal development and future careers.\n",
      "2. Organized structure: Schools typically have a hierarchy of administrators, teachers, and staff members.\n",
      "3. Curriculum: Schools offer various subjects and programs to help students gain a well-rounded education.\n",
      "4. Learning environment: Schools provide spaces and resources for students to learn and interact with each other.\n",
      "5. Institutional name: Many schools are named after their founders, locations, or based on their philosophy and practices, such as \"Loudoun Academy of Science\" or\n",
      "Listening...\n",
      "Silence detected, stopping...\n",
      "Transcribing...\n",
      "Goodbye detected. Ending conversation.\n"
     ]
    }
   ],
   "source": [
    "start_conversation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f437bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7897c30f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
